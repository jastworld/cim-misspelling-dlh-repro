{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y8wKHwwg17i9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14d03c2-37ee-47c7-e3da-82fdb0b3dde9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Mounted Google Drive to access files easily for each notebook session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS9jT6po2LnB",
        "outputId": "116cd612-56d6-44f6-ccb1-532736a92955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (185.1\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [W\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [W\r                                                                               \rHit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1,009 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,669 kB]\n",
            "Hit:11 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,150 kB]\n",
            "Hit:14 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Fetched 7,169 kB in 2s (3,922 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python3.8 is already the newest version (3.8.10-0ubuntu1~20.04.7).\n",
            "python3.8 set to manually installed.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n",
            "There are 2 choices for the alternative python3 (providing /usr/bin/python3).\n",
            "\n",
            "  Selection    Path                 Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/bin/python3.10   2         auto mode\n",
            "  1            /usr/bin/python3.10   2         manual mode\n",
            "  2            /usr/bin/python3.8    1         manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in manual mode\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python-pip-whl python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python-pip-whl python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 4 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 2,389 kB of archives.\n",
            "After this operation, 4,933 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.8 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 python3-setuptools all 45.2.0-1ubuntu0.1 [330 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-wheel all 0.34.2-1ubuntu0.1 [23.9 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.8 [231 kB]\n",
            "Fetched 2,389 kB in 1s (2,016 kB/s)\n",
            "Selecting previously unselected package python-pip-whl.\n",
            "(Reading database ... 122518 files and directories currently installed.)\n",
            "Preparing to unpack .../python-pip-whl_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Selecting previously unselected package python3-setuptools.\n",
            "Preparing to unpack .../python3-setuptools_45.2.0-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.34.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_20.0.2-5ubuntu1.8_all.deb ...\n",
            "Unpacking python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-setuptools (45.2.0-1ubuntu0.1) ...\n",
            "Setting up python3-wheel (0.34.2-1ubuntu0.1) ...\n",
            "Setting up python-pip-whl (20.0.2-5ubuntu1.8) ...\n",
            "Setting up python3-pip (20.0.2-5ubuntu1.8) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pip\n",
            "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 4.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "\u001b[33m  WARNING: The scripts pip, pip3, pip3.10 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed pip-23.1.2\n"
          ]
        }
      ],
      "source": [
        "#Downgrade Python to 3.8 to run requirements.txt smoothly\n",
        "\n",
        "!sudo apt-get update -y\n",
        "\n",
        "!sudo apt-get install python3.8\n",
        "\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 1\n",
        "\n",
        "!sudo update-alternatives --config python3\n",
        "\n",
        "!apt-get install python3-pip\n",
        "\n",
        "!python -m pip install --upgrade pip --user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4suvnpM12Tz8",
        "outputId": "29dedc44-9510-4083-9d79-36660113358e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.8.10\n"
          ]
        }
      ],
      "source": [
        "!python3 --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XX8oppVH3Y5k",
        "outputId": "787ac478-6b8b-435a-8723-13afdc9e3430"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.22.3 (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 1))\n",
            "  Downloading numpy-1.22.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.4.2 (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 2))\n",
            "  Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.64.0 (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 3))\n",
            "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==1.7.1 (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 4))\n",
            "  Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.2.0 (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading tensorflow-2.2.0-cp38-cp38-manylinux2010_x86_64.whl (516.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.2.0 (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading tensorboard-2.2.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairseq==0.9.0 (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 7))\n",
            "  Downloading fairseq-0.9.0.tar.gz (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.1/306.1 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==3.4.0 (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 8))\n",
            "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastDamerauLevenshtein==1.0.7 (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 9))\n",
            "  Downloading fastDamerauLevenshtein-1.0.7.tar.gz (36 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 10))\n",
            "  Downloading requests-2.30.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 11))\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlalchemy (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 12))\n",
            "  Downloading SQLAlchemy-2.0.12-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 13))\n",
            "  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.1 (from pandas==1.4.2->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 2))\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2020.1 (from pandas==1.4.2->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 2))\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions (from torch==1.7.1->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 4))\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting absl-py>=0.7.0 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astunparse==1.6.3 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting gast==0.3.3 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting google-pasta>=0.1.8 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading h5py-2.10.0-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.0 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf>=3.8.0 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading protobuf-4.22.4-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.3.0,>=2.2.0 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.6/454.6 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting wrapt>=1.11.1 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading wrapt-1.15.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six>=1.12.0 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting grpcio>=1.8.6 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading grpcio-1.54.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5)) (0.34.2)\n",
            "Collecting scipy==1.4.1 (from tensorflow==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 5))\n",
            "  Downloading scipy-1.4.1-cp38-cp38-manylinux1_x86_64.whl (26.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.0/26.0 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<2,>=1.6.3 (from tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting markdown>=2.6.8 (from tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6)) (45.2.0)\n",
            "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=0.11.15 (from tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading Werkzeug-2.3.3-py3-none-any.whl (242 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cffi (from fairseq==0.9.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 7))\n",
            "  Downloading cffi-1.15.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.7/442.7 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cython (from fairseq==0.9.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 7))\n",
            "  Using cached Cython-0.29.34-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (2.0 MB)\n",
            "Collecting regex (from fairseq==0.9.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 7))\n",
            "  Downloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu (from fairseq==0.9.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 7))\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers==0.9.2 (from transformers==3.4.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 8))\n",
            "  Downloading tokenizers-0.9.2-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging (from transformers==3.4.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 8))\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock (from transformers==3.4.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 8))\n",
            "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
            "Collecting sentencepiece!=0.1.92 (from transformers==3.4.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 8))\n",
            "  Downloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses (from transformers==3.4.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 8))\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting charset-normalizer<4,>=2 (from requests->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 10))\n",
            "  Downloading charset_normalizer-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.9/195.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5 (from requests->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 10))\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 10))\n",
            "  Downloading urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.2/123.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 10))\n",
            "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting greenlet!=0.4.17 (from sqlalchemy->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 12))\n",
            "  Downloading greenlet-2.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (618 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.5/618.5 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of gensim to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting gensim (from -r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 13))\n",
            "  Downloading gensim-4.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 13))\n",
            "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4 (from google-auth<2,>=1.6.3->tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=0.11.15->tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting pycparser (from cffi->fairseq==0.9.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 7))\n",
            "  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu->fairseq==0.9.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 7))\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tabulate>=0.8.9 (from sacrebleu->fairseq==0.9.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 7))\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting colorama (from sacrebleu->fairseq==0.9.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 7))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting lxml (from sacrebleu->fairseq==0.9.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 7))\n",
            "  Downloading lxml-4.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click (from sacremoses->transformers==3.4.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 8))\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zipp>=0.5 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->-r /content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt (line 6))\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fairseq, fastDamerauLevenshtein, sacremoses\n",
            "  Building wheel for fairseq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.9.0-cp38-cp38-linux_x86_64.whl size=1409839 sha256=c73a7bd13d43d76cfcf0b00a9ae69ed6a9bc5111f259af7b5e51b9a6e99a2f37\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/50/2e/2361c9438956b18032f5570526ebd0fab823545053a2835053\n",
            "  Building wheel for fastDamerauLevenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastDamerauLevenshtein: filename=fastDamerauLevenshtein-1.0.7-cp38-cp38-linux_x86_64.whl size=71880 sha256=2c61217caf2015737bc4c95f130a13a51738dd9d752214c805addfeca3964e29\n",
            "  Stored in directory: /root/.cache/pip/wheels/91/42/0c/b68ca2076f8935222ebee53cfc5c8c3aa8ad6046c8fddda14b\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895255 sha256=3ec90672e6f0960f661806a6f3a9aae055861b68309f5cf1d8503ae1e3c5b694\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built fairseq fastDamerauLevenshtein sacremoses\n",
            "Installing collected packages: tokenizers, tensorflow-estimator, tensorboard-plugin-wit, sentencepiece, pytz, fastDamerauLevenshtein, zipp, wrapt, urllib3, typing-extensions, tqdm, termcolor, tabulate, smart-open, six, regex, pycparser, pyasn1, protobuf, portalocker, packaging, oauthlib, numpy, MarkupSafe, lxml, joblib, idna, grpcio, greenlet, gast, filelock, cython, colorama, click, charset-normalizer, certifi, cachetools, absl-py, werkzeug, torch, sqlalchemy, scipy, sacremoses, sacrebleu, rsa, requests, python-dateutil, pyasn1-modules, opt-einsum, keras-preprocessing, importlib-metadata, h5py, google-pasta, cffi, astunparse, transformers, requests-oauthlib, pandas, markdown, google-auth, gensim, fairseq, google-auth-oauthlib, tensorboard, tensorflow\n",
            "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 cachetools-4.2.4 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-3.1.0 click-8.1.3 colorama-0.4.6 cython-0.29.34 fairseq-0.9.0 fastDamerauLevenshtein-1.0.7 filelock-3.12.0 gast-0.3.3 gensim-4.2.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 greenlet-2.0.2 grpcio-1.54.0 h5py-2.10.0 idna-3.4 importlib-metadata-6.6.0 joblib-1.2.0 keras-preprocessing-1.1.2 lxml-4.9.2 markdown-3.4.3 numpy-1.22.3 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.1 pandas-1.4.2 portalocker-2.7.0 protobuf-4.22.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycparser-2.21 python-dateutil-2.8.2 pytz-2023.3 regex-2023.5.5 requests-2.30.0 requests-oauthlib-1.3.1 rsa-4.9 sacrebleu-2.3.1 sacremoses-0.0.53 scipy-1.4.1 sentencepiece-0.1.99 six-1.16.0 smart-open-6.3.0 sqlalchemy-2.0.12 tabulate-0.9.0 tensorboard-2.2.0 tensorboard-plugin-wit-1.8.1 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-2.3.0 tokenizers-0.9.2 torch-1.7.1 tqdm-4.64.0 transformers-3.4.0 typing-extensions-4.5.0 urllib3-2.0.2 werkzeug-2.3.3 wrapt-1.15.0 zipp-3.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cffi",
                  "dateutil",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install requirements.txt\n",
        "!pip3 install -r \"/content/drive/MyDrive/cim-misspelling-dlh-repro/requirements.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "tlSWd7sA4f-M",
        "outputId": "b7d6dec2-e2f7-4356-b8b6-f52cdff93da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.*\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.22.4\n",
            "    Uninstalling protobuf-4.22.4:\n",
            "      Successfully uninstalled protobuf-4.22.4\n",
            "Successfully installed protobuf-3.20.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Downgrade protobuf for Colab requirements\n",
        "!pip3 install protobuf==3.20.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WHmGyHj42rI",
        "outputId": "29ec8631-07d4-47d3-f5fb-d43a54fca5e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tmimic_csv_dir: /content/drive/MyDrive/cim-misspelling-dlh-repro/data/mimic3/split\n",
            "\tdata_dir: data/dlh_multi_misspelling\n",
            "\tdict_file: data/lexicon/lexicon.json\n",
            "\tbert_dir: bert/ncbi_bert_base\n",
            "\toutput_dir: results/cim_base\n",
            "\tis_train: False\n",
            "\ttest_file: test.tsv\n",
            "\tinit_ckpt: results/cim_base/ckpt-475000.pkl\n",
            "\tinit_step: 475000\n",
            "\tseed: 123\n",
            "\tdecoder_layers: 12\n",
            "\ttrain_bert: False\n",
            "\tbert_finetune_factor: 1.0\n",
            "\tdropout: 0.1\n",
            "\tsynthetic_min_word_len: 3\n",
            "\tdo_substitution: True\n",
            "\tdo_transposition: True\n",
            "\tmax_word_corruptions: 2\n",
            "\tno_corruption_prob: 0.0\n",
            "\ttrain_with_ed: False\n",
            "\tbatch_size: 8\n",
            "\tnum_gpus: 2\n",
            "\toptimizer: adam\n",
            "\tadam_betas: (0.9, 0.999)\n",
            "\tadam_eps: 1e-08\n",
            "\tweight_decay: 0.01\n",
            "\ttraining_step: 200000\n",
            "\tdisplay_iter: 25\n",
            "\teval_iter: 2000\n",
            "\tlr_scheduler: inverse_sqrt\n",
            "\tlr: [0.0001]\n",
            "\twarmup_updates: 10000\n",
            "\twarmup_init_lr: 1e-06\n",
            "\tnum_beams: 30\n",
            "\tedit_distance_weight: 5.0\n",
            "\tedit_distance_extra_len: 1\n",
            "\tlength_penalty: 1.0\n",
            "\tbeam_sort_linear_ed: False\n",
            "\tbeam_final_score_normalize_ed: True\n",
            "\tdict_matching: True\n",
            "Read file /content/drive/MyDrive/cim-misspelling-dlh-repro/data/dlh_multi_misspelling/test.tsv... 30 rows\n",
            "Parsing rows (12 processes)\n",
            "100% 30/30 [00:00<00:00, 166.26it/s]\n",
            "Load model ckpt from results/cim_base/ckpt-475000.pkl (step 475000)\n",
            "numgpus 2\n",
            "DataParallelModel(\n",
            "  (module): CharacterLanguageModel(\n",
            "    (encoder): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (decoder): BartDecoder(\n",
            "      (embed_tokens): Embedding(60, 768, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(66, 768, padding_idx=1)\n",
            "      (layers): ModuleList(\n",
            "        (0): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (6): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (7): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (8): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (9): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (10): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (11): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=768, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test\n",
            "  0% 0/4 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "8\n",
            " 25% 1/4 [00:01<00:05,  1.81s/it]8\n",
            " 50% 2/4 [00:03<00:03,  1.58s/it]8\n",
            " 75% 3/4 [00:04<00:01,  1.43s/it]6\n",
            "100% 4/4 [00:05<00:00,  1.43s/it]\n",
            "Typo       : <s>brakiale</s>\n",
            "Correct    : <s>brachial</s>\n",
            "Transformer: ['<s>brachial</s>', '<s>brachials</s>', '<s>cranial</s>', '<s>brachialis</s>', '<s>brachially</s>', '<s>craniales</s>', '<s>radial</s>', '<s>cranially</s>', '<s>cranialized</s>', '<s>radiale</s>', '<s>cranialize</s>', '<s>cranialise</s>', '<s>cranialis</s>', '<s>basilar</s>', '<s>radials</s>', '<s>radiate</s>', '<s>cranialised</s>', '<s>brailled</s>', '<s>brainstem</s>', '<s>cranializes</s>', '<s>brachialides</s>', '<s>brailler</s>', '<s>cranialises</s>', '<s>radiology</s>', '<s>radiales</s>', '<s>branches</s>', '<s>radialize</s>', '<s>brachialgia</s>', '<s>radially</s>', '<s>brachia</s>']\n",
            "\n",
            "Typo       : <s>trakecha</s>\n",
            "Correct    : <s>trachea</s>\n",
            "Transformer: ['<s>trabecula</s>', '<s>trabeculae</s>', '<s>trabecular</s>', '<s>tracheobronchial</s>', '<s>trachea</s>', '<s>trabeculas</s>', '<s>traveling</s>', '<s>tracheomalacia</s>', '<s>trapezia</s>', '<s>treatment</s>', '<s>trabeculation</s>', '<s>tracheal</s>', '<s>travelling</s>', '<s>trabecule</s>', '<s>trabecularly</s>', '<s>tracheobronchomalacia</s>', '<s>tramadol</s>', '<s>treatments</s>', '<s>trabecularity</s>', '<s>transcervical</s>', '<s>trapezius</s>', '<s>tracheobronchitis</s>', '<s>trabeculum</s>', '<s>travel</s>', '<s>tracheobronchoplasty</s>', '<s>transcellular</s>', '<s>trabecules</s>', '<s>trabecularism</s>', '<s>trapezium</s>', '<s>transcutaneous</s>']\n",
            "\n",
            "Typo       : <s>pollypoesis</s>\n",
            "Correct    : <s>polyposis</s>\n",
            "Transformer: ['<s>polycythemia</s>', '<s>polyposis</s>', '<s>polycythemias</s>', '<s>polyphasia</s>', '<s>polypectomies</s>', '<s>polycythemic</s>', '<s>polycythemics</s>', '<s>polysomies</s>', '<s>polyplegia</s>', '<s>polyphasic</s>', '<s>polyplastic</s>', '<s>polyphonist</s>', '<s>polypodies</s>', '<s>polypectomy</s>', '<s>polyphonism</s>', '<s>polyphenic</s>', '<s>polyphonia</s>', '<s>polyphemic</s>', '<s>polypoides</s>', '<s>polypose</s>', '<s>polypeptins</s>', '<s>polypoidal</s>', '<s>polypoid</s>', '<s>polyphonies</s>', '<s>polyporthis</s>', '<s>polypeptides</s>', '<s>polyphase</s>', '<s>polypharmacy</s>', '<s>polypharmacist</s>', '<s>poly</s>']\n",
            "\n",
            "Typo       : <s>plurcy</s>\n",
            "Correct    : <s>pleurisy</s>\n",
            "Transformer: ['<s>pleural</s>', '<s>pleurisy</s>', '<s>pleuroscopy</s>', '<s>pleurae</s>', '<s>pleura</s>', '<s>pleurocele</s>', '<s>pleuritical</s>', '<s>pleuritically</s>', '<s>pleuro</s>', '<s>pleuritic</s>', '<s>pleurality</s>', '<s>pleuro-</s>', '<s>pleuritis</s>', '<s>purely</s>', '<s>pleuron</s>', '<s>pleuras</s>', '<s>pleurography</s>', '<s>pluggy</s>', '<s>pleuroid</s>', '<s>place</s>', '<s>pleuroscope</s>', '<s>pleurodesis</s>', '<s>pleuric</s>', '<s>pleurogenic</s>', '<s>pleurococcus</s>', '<s>pleurocentral</s>', '<s>pleurothoracic</s>', '<s>pleurocera</s>', '<s>pleurocentesis</s>', '<s>pleuroscopies</s>']\n",
            "\n",
            "Typo       : <s>dyufram</s>\n",
            "Correct    : <s>diaphragm</s>\n",
            "Transformer: ['<s>dysphagia</s>', '<s>defibrillator</s>', '<s>daughter</s>', '<s>deferred</s>', '<s>deflator</s>', '<s>difficult</s>', '<s>difficulty</s>', '<s>defibrillators</s>', '<s>deflated</s>', '<s>dysfunction</s>', '<s>daughter-in-law</s>', '<s>deformity</s>', '<s>defibrillation</s>', '<s>daughters</s>', '<s>dysphagic</s>', '<s>deflation</s>', '<s>defect</s>', '<s>dysfunctional</s>', '<s>suprapubic</s>', '<s>defibrillated</s>', '<s>deformities</s>', '<s>deflators</s>', '<s>deformed</s>', '<s>supraglottic</s>', '<s>defibrillates</s>', '<s>defibrillatory</s>', '<s>supraglottis</s>', '<s>defibrillate</s>', '<s>defibrillating</s>', '<s>during</s>']\n",
            "\n",
            "Total/Included/Correct = 30 / 18 / 13\n",
            "Save val results to /content/drive/MyDrive/cim-misspelling-dlh-repro/results/cim_large/test-475000/results_synthetic_test_beam30_ed1-5.0_lp1.0_bsFalse_fsTrue_dict.pkl\n",
            "100% 4/4 [00:00<00:00,  5.29it/s]\n",
            "Save val results to /content/drive/MyDrive/cim-misspelling-dlh-repro/results/cim_large/test-475000correct_synthetic_test_beam30_ed1-5.0_lp1.0_bsFalse_fsTrue_dict.pkl\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "# Run eval_cim_base\n",
        "!bash /content/drive/MyDrive/cim-misspelling-dlh-repro/eval_cim_base.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o3xzaXcL8GM",
        "outputId": "d1602098-2137-4252-dda1-3b57ca64c9ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tmimic_csv_dir: /content/drive/MyDrive/cim-misspelling-dlh-repro/data/mimic3/split\n",
            "\tdata_dir: data/dlh_multi_misspelling\n",
            "\tdict_file: data/lexicon/lexicon.json\n",
            "\tbert_dir: bert/ncbi_bert_large\n",
            "\toutput_dir: results/cim_large\n",
            "\tis_train: False\n",
            "\ttest_file: test.tsv\n",
            "\tinit_ckpt: results/cim_large/ckpt-300000.pkl\n",
            "\tinit_step: 300000\n",
            "\tseed: 123\n",
            "\tdecoder_layers: 24\n",
            "\ttrain_bert: False\n",
            "\tbert_finetune_factor: 1.0\n",
            "\tdropout: 0.1\n",
            "\tsynthetic_min_word_len: 3\n",
            "\tdo_substitution: True\n",
            "\tdo_transposition: True\n",
            "\tmax_word_corruptions: 2\n",
            "\tno_corruption_prob: 0.0\n",
            "\ttrain_with_ed: False\n",
            "\tbatch_size: 6\n",
            "\tnum_gpus: 2\n",
            "\toptimizer: adam\n",
            "\tadam_betas: (0.9, 0.999)\n",
            "\tadam_eps: 1e-08\n",
            "\tweight_decay: 0.01\n",
            "\ttraining_step: 200000\n",
            "\tdisplay_iter: 25\n",
            "\teval_iter: 2000\n",
            "\tlr_scheduler: inverse_sqrt\n",
            "\tlr: [0.0001]\n",
            "\twarmup_updates: 10000\n",
            "\twarmup_init_lr: 1e-06\n",
            "\tnum_beams: 30\n",
            "\tedit_distance_weight: 5.0\n",
            "\tedit_distance_extra_len: 1\n",
            "\tlength_penalty: 1.0\n",
            "\tbeam_sort_linear_ed: False\n",
            "\tbeam_final_score_normalize_ed: True\n",
            "\tdict_matching: True\n",
            "Read file /content/drive/MyDrive/cim-misspelling-dlh-repro/data/dlh_multi_misspelling/test.tsv... 30 rows\n",
            "Parsing rows (12 processes)\n",
            "100% 30/30 [00:00<00:00, 183.52it/s]\n",
            "Load model ckpt from results/cim_large/ckpt-300000.pkl (step 300000)\n",
            "numgpus 2\n",
            "DataParallelModel(\n",
            "  (module): CharacterLanguageModel(\n",
            "    (encoder): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 1024)\n",
            "        (token_type_embeddings): Embedding(2, 1024)\n",
            "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (12): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (13): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (14): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (15): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (16): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (17): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (18): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (19): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (20): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (21): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (22): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (23): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (decoder): BartDecoder(\n",
            "      (embed_tokens): Embedding(60, 1024, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(66, 1024, padding_idx=1)\n",
            "      (layers): ModuleList(\n",
            "        (0): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (1): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (2): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (3): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (4): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (5): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (6): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (7): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (8): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (9): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (10): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (11): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (12): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (13): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (14): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (15): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (16): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (17): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (18): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (19): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (20): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (21): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (22): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "        (23): DecoderLayer(\n",
            "          (self_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder_attn): Attention(\n",
            "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Test\n",
            "  0% 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
            "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
            "  return torch.floor_divide(self, other)\n",
            "6\n",
            " 20% 1/5 [00:02<00:09,  2.32s/it]6\n",
            " 40% 2/5 [00:04<00:06,  2.14s/it]6\n",
            " 60% 3/5 [00:06<00:03,  1.93s/it]6\n",
            " 80% 4/5 [00:07<00:01,  1.87s/it]6\n",
            "100% 5/5 [00:10<00:00,  2.02s/it]\n",
            "Typo       : <s>prostait</s>\n",
            "Correct    : <s>prostate</s>\n",
            "Transformer: ['<s>prostatitis</s>', '<s>prostate</s>', '<s>prostatic</s>', '<s>prostatitic</s>', '<s>prosthesis</s>', '<s>pulsatile</s>', '<s>prosthetic</s>', '<s>prostat</s>', '<s>posterior</s>', '<s>protocol</s>', '<s>pulsation</s>', '<s>post-ictal</s>', '<s>post-op</s>', '<s>pulsatility</s>', '<s>restriction</s>', '<s>prostats</s>', '<s>prostates</s>', '<s>post</s>', '<s>protein</s>', '<s>pulsations</s>', '<s>positive</s>', '<s>position</s>', '<s>restricted</s>', '<s>postact</s>', '<s>restrict</s>', '<s>prosthetics</s>', '<s>prostatism</s>', '<s>restrictive</s>', '<s>prostheses</s>', '<s>probable</s>']\n",
            "\n",
            "Typo       : <s>kluhmideeuh</s>\n",
            "Correct    : <s>chlamydia</s>\n",
            "Transformer: ['<s>klebsiella</s>', '<s>fluctuation</s>', '<s>fluctuating</s>', '<s>florid</s>', '<s>sluggishness</s>', '<s>fluctuates</s>', '<s>fluid</s>', '<s>fluctuations</s>', '<s>clumped</s>', '<s>fluctuated</s>', '<s>fluconazole</s>', '<s>floridness</s>', '<s>sluggish</s>', '<s>fluids</s>', '<s>fluoroscopy</s>', '<s>sluggishly</s>', '<s>clumsiness</s>', '<s>klebsiellae</s>', '<s>fluctuance</s>', '<s>fluoroscopic</s>', '<s>fluctuate</s>', '<s>fluctuantly</s>', '<s>clumsinesses</s>', '<s>slumped</s>', '<s>fluctuational</s>', '<s>flushed</s>', '<s>flushing</s>', '<s>sluggishnesses</s>', '<s>fluoroscopies</s>', '<s>fluoroscoped</s>']\n",
            "\n",
            "Typo       : <s>ooforektomy</s>\n",
            "Correct    : <s>oophorectomy</s>\n",
            "Transformer: ['<s>colonoscopy</s>', '<s>colorectally</s>', '<s>colorectal</s>', '<s>nonoperative</s>', '<s>nonoperatively</s>', '<s>colostomy</s>', '<s>nonrebreather</s>', '<s>cooperation</s>', '<s>nonrebreathing</s>', '<s>nonresponsive</s>', '<s>cooperative</s>', '<s>colorectum</s>', '<s>nonreactive</s>', '<s>colonoscope</s>', '<s>cooperations</s>', '<s>nonreaction</s>', '<s>colonoscopies</s>', '<s>colonoscopic</s>', '<s>cooperatively</s>', '<s>colorectums</s>', '<s>nonresponding</s>', '<s>nonrelation</s>', '<s>donor</s>', '<s>colonoscopically</s>', '<s>operation</s>', '<s>operations</s>', '<s>nonrebreathers</s>', '<s>nonresponsively</s>', '<s>nonreversed</s>', '<s>nonreactions</s>']\n",
            "\n",
            "Typo       : <s>optomology</s>\n",
            "Correct    : <s>ophthalmology</s>\n",
            "Transformer: ['<s>opthalmology</s>', '<s>otolaryngology</s>', '<s>orthology</s>', '<s>pulmonology</s>', '<s>opthalmologic</s>', '<s>optometry</s>', '<s>oncology</s>', '<s>optology</s>', '<s>otolaryngologist</s>', '<s>otolaryngologic</s>', '<s>otolaryngologies</s>', '<s>pulmonary</s>', '<s>otolaryngologists</s>', '<s>pulmonologist</s>', '<s>protocolist</s>', '<s>optically</s>', '<s>optimally</s>', '<s>pulmonologists</s>', '<s>optometries</s>', '<s>pulmonologic</s>', '<s>otolaryngological</s>', '<s>protocols</s>', '<s>optometric</s>', '<s>protocol</s>', '<s>oncologist</s>', '<s>orthological</s>', '<s>ototoxicity</s>', '<s>optometrist</s>', '<s>orthopedics</s>', '<s>orthologs</s>']\n",
            "\n",
            "Typo       : <s>tunsols</s>\n",
            "Correct    : <s>tonsils</s>\n",
            "Transformer: ['<s>tonsils</s>', '<s>tumors</s>', '<s>tunnels</s>', '<s>tendons</s>', '<s>tissues</s>', '<s>muscles</s>', '<s>tensors</s>', '<s>fungals</s>', '<s>tumours</s>', '<s>tonsillar</s>', '<s>tenses</s>', '<s>tendonitis</s>', '<s>fundoplications</s>', '<s>tunneled</s>', '<s>tonsilla</s>', '<s>fundoscopies</s>', '<s>tonsil</s>', '<s>tonsillitis</s>', '<s>tunneling</s>', '<s>fungal</s>', '<s>tonsillectomies</s>', '<s>tunnel</s>', '<s>tunnelled</s>', '<s>tonsilectomies</s>', '<s>tonsile</s>', '<s>fundoscopic</s>', '<s>fundoplication</s>', '<s>tonsillo</s>', '<s>muscle</s>', '<s>understands</s>']\n",
            "\n",
            "Total/Included/Correct = 30 / 15 / 10\n",
            "Save val results to /content/drive/MyDrive/cim-misspelling-dlh-repro/results/cim_large/test-300000/results_synthetic_test_beam30_ed1-5.0_lp1.0_bsFalse_fsTrue_dict.pkl\n",
            "100% 5/5 [00:01<00:00,  4.44it/s]\n",
            "Save val results to /content/drive/MyDrive/cim-misspelling-dlh-repro/results/cim_large/test-300000correct_synthetic_test_beam30_ed1-5.0_lp1.0_bsFalse_fsTrue_dict.pkl\n",
            "done\n"
          ]
        }
      ],
      "source": [
        "# Run eval_cim_large\n",
        "!bash /content/drive/MyDrive/cim-misspelling-dlh-repro/eval_cim_large.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P25AKg9GCN0D",
        "outputId": "d5e91846-f97b-4906-e99d-8c2c83fe9e01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'example_id': 0, 'note_id': 52849, 'typo': '<s>carediolgy</s>', 'correct': '<s>cardiology</s>', 'output': ['<s>cardiology</s>', '<s>cardiologist</s>', '<s>cariology</s>', '<s>cardiologists</s>', '<s>radiology</s>', '<s>electrophysiology</s>', '<s>cardiologies</s>', '<s>anesthesiology</s>', '<s>carcinology</s>', '<s>cardiol</s>', '<s>cardiologic</s>', '<s>previously</s>', '<s>cardiovascular</s>', '<s>cardiological</s>', '<s>cardiomegaly</s>', '<s>cardiologically</s>', '<s>carvedilol</s>', '<s>care</s>', '<s>electrophysiologist</s>', '<s>electrophysiologists</s>', '<s>caregivers</s>', '<s>cardio</s>', '<s>cardiomyopathy</s>', '<s>cardiopulmonary</s>', '<s>cardiopathy</s>', '<s>cardio-oncology</s>', '<s>cardiolith</s>', '<s>cardiovascularly</s>', '<s>radiologists</s>', '<s>paramedically</s>', '<s>cardiovasculars</s>', '<s>attendings</s>', '<s>anesthesiologist</s>', '<s>particularly</s>', '<s>cardio-pulmonary</s>', '<s>attendingly</s>', '<s>anesthesiologists</s>', '<s>directions</s>', '<s>additionally</s>', '<s>electrology</s>', '<s>cardio-surgery</s>', '<s>appropriately</s>', '<s>electrophysiologic</s>', '<s>particular</s>', '<s>radiologist</s>', '<s>approximately</s>', '<s>electrophysiologies</s>', '<s>cardiotherapy</s>', '<s>caretakers</s>', '<s>cardiolipid</s>', '<s>participants</s>', '<s>cardiography</s>', '<s>carcinologist</s>', '<s>cardiotomy</s>', '<s>cardiolysis</s>', '<s>cardiograms</s>', '<s>carefully</s>', '<s>caregiving</s>', '<s>cardioids</s>', '<s>cardiothoracic</s>', '<s>specially</s>', '<s>attending</s>', '<s>participant</s>', '<s>cardiovasculature</s>', '<s>premedication</s>', '<s>cardiosurgery</s>', '<s>gradually</s>', '<s>specialists</s>', '<s>scheduled</s>', '<s>premedications</s>', '<s>directionally</s>', '<s>cardiovasculare</s>', '<s>participation</s>', '<s>cardiopath</s>', '<s>paramedics</s>', '<s>cardioversion</s>', '<s>candidacy</s>', '<s>anesthesiologies</s>', '<s>pediatricians</s>', '<s>preparations</s>', '<s>electrophysiological</s>', '<s>electrophoresis</s>', '<s>medically</s>', '<s>particles</s>', '<s>scheduling</s>', '<s>preoperatively</s>', '<s>cardiomyopathies</s>', '<s>careeringly</s>', '<s>electrophonically</s>', '<s>cardio-</s>', '<s>cards</s>', '<s>pre-operatively</s>', '<s>responsibility</s>', '<s>partially</s>', '<s>cardiac</s>', '<s>particulars</s>', '<s>cardiolipin</s>', '<s>speciality</s>', '<s>anesthesiologic</s>', '<s>participating</s>', '<s>care-givers</s>', '<s>participatory</s>', '<s>cardiovasculopathy</s>', '<s>directionality</s>', '<s>specialty</s>', '<s>cardio-surgical</s>', '<s>appointments</s>', '<s>specialist</s>', '<s>cardioangiology</s>', '<s>previous</s>', '<s>cardiorrhaphy</s>', '<s>electrophores</s>', '<s>electrophoretically</s>', '<s>cardiogram</s>', '<s>cardio-regulatory</s>', '<s>cardiacally</s>', '<s>electrophilically</s>', '<s>predisposing</s>', '<s>residentially</s>', '<s>cardiofacial</s>', '<s>cardiovert</s>', '<s>anesthesiologically</s>', '<s>cardiopathic</s>', '<s>cardio-renal</s>', '<s>cardiomyoplasty</s>', '<s>participantly</s>', '<s>radiologically</s>', '<s>cardioverts</s>', '<s>providers</s>', '<s>cardioid</s>', '<s>precautions</s>', '<s>participations</s>', '<s>cardiotropic</s>', '<s>cardiomypathy</s>', '<s>parenting</s>', '<s>precautionary</s>', '<s>cardioscope</s>', '<s>targeting</s>', '<s>premedicates</s>', '<s>cardioscopically</s>', '<s>cardioplasty</s>', '<s>themselves</s>', '<s>cardiocytes</s>', '<s>participate</s>', '<s>specializations</s>', '<s>electrolytes</s>', '<s>carcinologically</s>', '<s>prescriptions</s>', '<s>hospitalizations</s>', \"<s>direction's</s>\", '<s>respiratory</s>', '<s>premedicate</s>', '<s>cardiovasology</s>', \"<s>preparation's</s>\", '<s>cardioverting</s>', '<s>specialties</s>', '<s>cardioversions</s>', '<s>cardiotoxicity</s>', '<s>radiologies</s>', '<s>cardiographs</s>', '<s>prednisone</s>', '<s>preparatively</s>', '<s>premedicated</s>', '<s>participates</s>', '<s>cardiotherapies</s>', '<s>electrophorectically</s>', '<s>directors</s>', '<s>cardio-respiratory</s>', '<s>parameters</s>', '<s>caregiver</s>', '<s>variously</s>', '<s>graduations</s>', '<s>directory</s>', '<s>electrophysically</s>', '<s>pediatrician</s>', '<s>cardio-embolism</s>', '<s>electrophorese</s>', '<s>additional</s>', '<s>cardio-angiology</s>', '<s>cardioverters</s>', '<s>cardiopaths</s>', '<s>schedules</s>', '<s>cardiopathies</s>', '<s>cardiograph</s>', '<s>electrologists</s>', '<s>electrolytically</s>', '<s>earlier</s>', '<s>paramedical</s>', '<s>preparation</s>', '<s>certifications</s>', '<s>appointment</s>', '<s>prevident</s>', '<s>medications</s>', '<s>premedicants</s>', '<s>preventions</s>', '<s>responsibilities</s>', '<s>premedicating</s>', '<s>anesthesiological</s>', '<s>radiological</s>', '<s>electrophoreses</s>', '<s>predisposition</s>', '<s>cardio-vascular</s>', '<s>specialization</s>', '<s>carding</s>', '<s>carioling</s>', '<s>careering</s>', '<s>prescribers</s>', '<s>particularity</s>', '<s>scheduler</s>', '<s>cardiopathia</s>', '<s>prescription</s>', '<s>cardioscopic</s>', '<s>cardioscopes</s>', '<s>specialities</s>', '<s>electropharmacology</s>', '<s>premorbidities</s>', '<s>cariously</s>', '<s>directional</s>', '<s>prepiously</s>', '<s>cardiolite</s>', \"<s>participant's</s>\", '<s>cardio-surgeries</s>', '<s>hospitalization</s>', '<s>hospitality</s>', '<s>questioning</s>', '<s>cardiotomies</s>', '<s>electrologist</s>', '<s>predisposes</s>', \"<s>specialization's</s>\", '<s>cardio-embolic</s>', '<s>certification</s>', '<s>cardiographies</s>', '<s>schedulers</s>', '<s>caretaking</s>', '<s>radiologic</s>', '<s>prematurity</s>', '<s>parents</s>', '<s>questions</s>', '<s>cardiopulmonarily</s>', '<s>carcinologic</s>', '<s>cardiophonically</s>', '<s>cardially</s>', '<s>cardiometers</s>', '<s>cardio-embolisms</s>', '<s>cardiovasculopathies</s>', '<s>carcinological</s>', '<s>certificates</s>', '<s>schedule</s>', '<s>predispositions</s>', '<s>premedicant</s>', '<s>arrangements</s>', '<s>residents</s>', '<s>cardiomediastinal</s>', '<s>approximators</s>', '<s>participator</s>', '<s>previousness</s>', '<s>precautioning</s>', '<s>electrophiles</s>', '<s>paramedicals</s>', '<s>cardiorespiratory</s>', '<s>carcinoma</s>', '<s>cardiovalvular</s>', '<s>preventional</s>', '<s>hospitalists</s>', '<s>cardiometer</s>', '<s>cardiolipins</s>', '<s>prednisones</s>', '<s>cardium</s>', '<s>electrophonic</s>', '<s>electrophore</s>', '<s>participatively</s>', '<s>stressors</s>', '<s>cardis</s>', '<s>cardiographers</s>', '<s>cardial</s>', '<s>cardioverter</s>', '<s>specializer</s>', '<s>cardiotoxic</s>', \"<s>parkinson's</s>\", '<s>certificate</s>', '<s>cardiophrenia</s>', '<s>pregnancy</s>', '<s>several</s>', '<s>cardiographic</s>', '<s>referrals</s>', '<s>additions</s>', '<s>crediting</s>', '<s>prematurities</s>', '<s>electrolarynx</s>', '<s>medical</s>', '<s>cardioblast</s>', '<s>hospitalist</s>', '<s>participators</s>', '<s>cardiosurgical</s>', '<s>approximate</s>', '<s>pediatrics</s>', '<s>cardio-facial</s>', '<s>direction</s>', '<s>certificatory</s>', '<s>participative</s>'], 'scores': [-1.5792104547674006, -2.7151559682992787, -2.792347717285156, -2.836045673915318, -3.09007568359375, -3.2144911024305554, -3.2733524029071512, -3.519364929199219, -3.522423426310221, -3.550614595413208, -3.594883600870768, -3.6350257179953833, -3.6460525512695314, -3.7026571546282088, -3.739821507380559, -3.7698588371276855, -3.77408183704723, -3.781838226318359, -3.7950958251953124, -3.835353669666109, -3.8389930725097656, -3.852878843035017, -3.986522165934245, -4.00140380859375, -4.016797701517741, -4.036473751068115, -4.068165172230113, -4.071313297047334, -4.084393134483924, -4.0888671875, -4.090930461883545, -4.09820591319691, -4.09965919045841, -4.1070685753455525, -4.109444113338695, -4.119366963704427, -4.145765092637804, -4.163050911643288, -4.170139606182392, -4.171716690063477, -4.182630157470703, -4.192814418247768, -4.202786897358141, -4.203300822864879, -4.214736302693685, -4.235098702566964, -4.242024993896484, -4.256482260567801, -4.256943442604759, -4.257031758626302, -4.257864145132212, -4.273035489595854, -4.290918350219727, -4.291973460804332, -4.299228668212891, -4.324227015177409, -4.325989151000977, -4.328942732377485, -4.349092864990235, -4.355664571126302, -4.3559223175048825, -4.356231689453125, -4.360349337259929, -4.372321234809028, -4.37885502406529, -4.379425048828125, -4.382018280029297, -4.386308670043945, -4.394877624511719, -4.405846150716146, -4.4117633274623325, -4.418062686920166, -4.427133287702288, -4.446232882412997, -4.447675531560725, -4.465020315987723, -4.466727447509766, -4.470844941980698, -4.476362773350307, -4.481547135573167, -4.485586620512462, -4.487717151641846, -4.491323089599609, -4.4946941375732425, -4.496535214510831, -4.497989400227865, -4.501255259794347, -4.501328468322754, -4.503038618299696, -4.503932952880859, -4.511852264404297, -4.5129475593566895, -4.519143676757812, -4.523109817504883, -4.528779029846191, -4.532368342081706, -4.534806569417317, -4.538184772838246, -4.538578987121582, -4.5423687526157925, -4.542551040649414, -4.550224304199219, -4.5522215993780835, -4.553677368164062, -4.555107498168946, -4.555610179901123, -4.557986919696514, -4.567075209184126, -4.567452430725098, -4.582621256510417, -4.587671552385602, -4.588076455252511, -4.592698669433593, -4.600765921852806, -4.604022555881077, -4.611948649088542, -4.618594699435764, -4.625888237586389, -4.633043561662946, -4.633186927208533, -4.633315346457741, -4.633441925048828, -4.635715191180889, -4.639251708984375, -4.641048431396484, -4.641801016671317, -4.643304443359375, -4.64833132425944, -4.648516082763672, -4.651337517632379, -4.660641670227051, -4.661328125, -4.669217036320613, -4.672362191336496, -4.673285675048828, -4.673859187534878, -4.674025217692058, -4.676974487304688, -4.68417241023137, -4.6853022855870865, -4.687506455641526, -4.69060585715554, -4.694394429524739, -4.697135925292969, -4.6972808837890625, -4.6981377234825725, -4.698664496926701, -4.699832371303013, -4.702111188103171, -4.702243169148763, -4.702337582906087, -4.702515602111816, -4.705912780761719, -4.710552215576172, -4.711212158203125, -4.713397026062012, -4.713520304361979, -4.718176778157552, -4.718211491902669, -4.719210698054387, -4.742892178622159, -4.743278503417969, -4.743473933293269, -4.745570256159856, -4.746670722961426, -4.751856849307106, -4.753415298461914, -4.75475391588713, -4.763166601007635, -4.765879440307617, -4.766854095458984, -4.771775245666504, -4.774946975708008, -4.775154961480035, -4.781246185302734, -4.787412643432617, -4.787445068359375, -4.78763164173473, -4.789119944852941, -4.790502820696149, -4.790630340576172, -4.793721008300781, -4.7955780029296875, -4.79829470316569, -4.801775105794271, -4.803472182329963, -4.814894199371338, -4.815547625223796, -4.8155562082926435, -4.8157506306966145, -4.819032669067383, -4.821991729736328, -4.822081247965495, -4.822509178748498, -4.827402114868164, -4.8286756627699905, -4.8313189915248325, -4.8331298828125, -4.833584418663611, -4.842178821563721, -4.842914835611979, -4.845129013061523, -4.847069803873698, -4.851009845733643, -4.854938507080078, -4.855685424804688, -4.861084938049316, -4.864672524588449, -4.866452026367187, -4.867226233849158, -4.867726839505709, -4.875360342172476, -4.875555478609526, -4.875755310058594, -4.879494476318359, -4.884034220377604, -4.886397933959961, -4.886900901794434, -4.887481689453125, -4.890729037198153, -4.897735595703125, -4.898822560029871, -4.900920391082764, -4.901795069376628, -4.9028275807698565, -4.904581510103666, -4.905614580426898, -4.906213760375977, -4.9072095085592835, -4.908345540364583, -4.914173671177456, -4.915367126464844, -4.917087901722301, -4.919009815562855, -4.91961669921875, -4.920004208882649, -4.922170639038086, -4.9261932373046875, -4.928821139865452, -4.935089698204627, -4.935221952550552, -4.937845993041992, -4.939105694110577, -4.939542433794807, -4.9437749953497026, -4.944938151041667, -4.945023169884315, -4.949267917209202, -4.9533514976501465, -4.9536135991414385, -4.956826430100661, -4.960859298706055, -4.96464369032118, -4.9651674543108255, -4.975764347956731, -4.978911766639123, -4.984070914132254, -4.987448011125837, -4.989800673264724, -4.990676456027561, -4.991656494140625, -4.994355773925781, -4.994390634390024, -4.994969294621394, -4.998257954915364, -4.9994635948768025, -4.999876022338867, -5.00244140625, -5.002595084054129, -5.010868365948017, -5.013565540313721, -5.015164184570312, -5.017289297921317, -5.018039957682292, -5.018467903137207, -5.018533559945913, -5.019431749979655, -5.020002047220866, -5.020151138305664, -5.020376205444336, -5.020811898367746, -5.030587005615234, -5.031619071960449, -5.031651088169643, -5.0369873046875, -5.039876937866211, -5.042764282226562, -5.043534415108817, -5.045126778738839, -5.047917366027832, -5.049581845601399, -5.049691518147786, -5.050768715994699, -5.051658121744792, -5.052714029947917, -5.054332039572976, -5.056621551513672, -5.057602310180664, -5.057975769042969, -5.061860765729632], 'lm_scores': [-0.6701195023276589, -0.7920791185819186, -1.2923476219177246, -0.6931885991777692, -1.0900755882263184, -0.43671340412563747, -1.3502754798302283, -0.8526982625325521, -1.8557569185892742, -2.300614595413208, -1.928217093149821, -0.907752817327326, -0.9793858846028646, -1.5598000117710658, -1.8167445843036358, -1.5823588371276855, -1.9559000188654119, -1.7818380355834962, -0.5450957298278809, -0.5020202000935873, -1.566265799782493, -2.424307414463588, -1.3198554992675782, -1.50140380859375, -1.9334643681844075, -1.8489738702774048, -2.2499831806529653, -1.424254249123966, -1.0074701309204102, -1.9460101808820451, -1.2784305810928345, -0.9163878180763938, -0.8643649606143727, -1.030145498422476, -1.4623855142032398, -1.619366963704427, -0.8124319182501899, -0.9812327298251066, -1.0932163825401893, -1.2550501028696697, -1.8492968241373697, -0.6213858468191964, -1.0448923612895764, -1.0214827277443626, -1.2980697949727376, -0.6636699948992048, -0.9920248985290527, -1.7564822605678014, -1.529670541936701, -2.1736982663472495, -0.7963256102341872, -1.9653433286226714, -1.7909183502197266, -2.4737918160178443, -2.215895334879557, -1.8242268562316895, -2.3259891510009765, -2.056215459650213, -2.3490930557250977, -1.3556646982828775, -1.355922317504883, -0.8562314987182618, -1.027016003926595, -1.3167656792534723, -0.8074265888759068, -2.23656804221017, -1.3820181846618653, -0.6363085508346558, -0.8948777198791504, -0.7391792297363281, -1.9117631912231445, -1.6055625677108765, -0.8557048525129046, -2.1735057830810547, -1.2658572630448774, -1.607877322605678, -1.9667274475097656, -1.2355510487275965, -0.5477912425994873, -1.0200087473942683, -1.1522532871791296, -0.7377170920372009, -1.4913230895996095, -1.494694137573242, -0.8601714914495294, -0.8313227971394856, -1.2659611421472885, -2.8346618016560874, -1.169705284966363, -2.6289327144622803, -2.011852264404297, -0.7629473805427551, -0.8524768193562825, -2.0231098175048827, -2.0287787914276123, -1.1990351676940918, -2.451473077138265, -1.356366591020064, -1.413578987121582, -0.9709403174264091, -2.042551040649414, -1.3359384536743164, -1.6574846568860506, -1.8870107014973958, -1.555107593536377, -1.7431102991104126, -0.32721776228684646, -0.9307115728204901, -2.3799524307250977, -1.2492879231770833, -2.087671688624791, -1.0166477475847517, -1.3426986694335938, -2.3280386491255327, -1.8262445661756728, -2.528615156809489, -1.2852613661024306, -1.5489651606633112, -1.4187581198556083, -1.940879381619967, -2.3605880737304688, -1.3834421157836914, -1.9434073521540716, -1.9469441633958082, -2.1410481929779053, -1.4275155748639787, -1.6433045705159506, -2.14833132425944, -0.6485161781311035, -2.429115507337782, -0.910641590754191, -0.9946615854899089, -1.9769093440129206, -2.1723623275756836, -1.673285675048828, -1.459573473249163, -2.174025217692057, -1.6769742965698242, -1.2226340220524714, -2.038243462057675, -2.7644295325646033, -0.5996967228976163, -2.1943944295247397, -1.3638025919596355, -0.947280764579773, -0.8519838773287259, -2.0516056733972885, -0.7712611470903669, -0.5844640731811523, -1.785576343536377, -0.9523375034332275, -1.3691822687784831, -2.705912780761719, -1.4962665012904577, -1.854069437299456, -0.9633970260620117, -1.7135201772054036, -2.0515101114908854, -1.8015448252360027, -2.0269030057466946, -1.561073823408647, -1.5289925166538783, -1.2819353250356822, -1.2840317946213942, -1.6216704845428467, -1.4185234251476468, -1.2534153938293457, -1.5968591790450246, -1.1268029646439985, -2.265879440307617, -1.7668542861938477, -1.0217751661936443, -1.7749469757080079, -1.4418217341105144, -0.550476991213285, -1.9749127626419067, -1.1207785288492838, -1.1512678319757634, -2.436178543988396, -1.9333599635532923, -2.290630499521891, -1.2937210083007813, -1.9384354182652064, -2.2982948621114097, -1.1351083119710286, -1.5681781768798828, -1.0648943185806274, -1.898880958557129, -1.065556287765503, -1.1490840911865234, -0.6523659229278564, -1.3219917297363282, -0.6554145415623983, -1.3609705704909105, -1.077402114868164, -0.711028772241929, -1.2598902838570731, -1.4997966554429796, -1.7566613417405348, -1.0921788215637207, -1.5095816294352213, -2.0326287746429443, -0.8470697402954102, -2.3510098457336426, -2.354938507080078, -2.3556854248046877, -1.111085017522176, -1.6503869465419225, -1.3664522171020508, -2.1749185415414662, -1.0215730667114258, -2.183052503145658, -2.183247786301833, -1.0296013905451848, -1.8794944763183594, -1.2173672993977864, -2.386397933959961, -1.9702342351277669, -2.1602087887850674, -3.0725472190163354, -1.3263067517961775, -1.9576458650476791, -0.5259202122688293, -1.1517949104309082, -1.1528276602427165, -2.2122738177959738, -1.3341858727591378, -1.9895470937093098, -1.3777976316564224, -2.2416791280110675, -1.342745099748884, -1.9153671264648438, -1.2807241786609997, -2.191737088290128, -2.1923437985506924, -1.17000412940979, -1.172170877456665, -0.9261933326721191, -2.1510433620876737, -2.6273973905123196, -2.2881631290211395, -3.437845993041992, -2.2467982952411356, -1.9983659632065718, -1.610441662016369, -2.278271230061849, -1.483484855064979, -1.0603788163926866, -1.515851616859436, -1.6202802658081055, -0.7260571993314303, -0.9608592987060547, -1.909088134765625, -1.0365960938589913, -1.514226033137395, -1.517373305100661, -1.412642478942871, -1.4160195759364538, -2.2974928342379055, -1.9351209004720051, -1.9916563034057617, -2.3276891072591144, -1.5328524662898138, -0.7642002105712891, -2.498257795969645, -2.691771287184495, -2.0832093556722007, -2.502441644668579, -1.431166512625558, -1.1647145931537335, -1.5760654211044312, -1.5151641845703125, -2.874432155064174, -2.018039830525716, -3.143468141555786, -2.3262257209190955, -1.2694316705067952, -2.5200020472208657, -1.6868176460266113, -1.6870428721110027, -2.163668768746512, -1.5305868148803712, -0.6566190719604492, -2.1745079585484097, -1.0369872093200683, -1.0398770332336427, -2.5427644729614256, -1.1149628502982003, -1.4736979348318917, -1.2979176044464111, -2.9662485122680664, -0.8830249309539795, -1.4793402808053153, -2.384991200764974, -1.3027138710021973, -0.96342303536155, -2.1994785581316267, -1.5576024055480957, -1.8436899185180664, -1.4904320580618722]}\n"
          ]
        }
      ],
      "source": [
        "# Read first entry of output pickle file with full potential correct words array (I stored in Google Drive)\n",
        "import pandas as pd\n",
        "obj = pd.read_pickle(\"/content/drive/MyDrive/cim-misspelling-dlh-repro/results/cim_base/test-475000/results_synthetic_test_beam300_ed1-5.0_lp1.0_bsFalse_fsTrue_dict.pkl\")\n",
        "print(obj[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "874TvD1xCj6K",
        "outputId": "8233ad59-ae5a-473c-e74d-4ca828ffb63f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'example_id': 1, 'note_id': 393217, 'typo': '<s>ztracking</s>', 'correct': '<s>tracking</s>', 'score': -1.137838363647461, 'lm_score': -0.5822828080919054}\n"
          ]
        }
      ],
      "source": [
        "# Read first entry of output pickle file with matched word\n",
        "import pandas as pd\n",
        "obj = pd.read_pickle(\"/content/cim-misspelling-dlh-repro/results/cim_base/test-475000correct_synthetic_test_beam30_ed1-5.0_lp1.0_bsFalse_fsTrue_dict.pkl\")\n",
        "print(obj[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}