{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch fairseq transformers dill fastDamerauLevenshtein tensorboardX accelerate textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from model.word_lm import SpellCorrectionModel\n",
    "from model.char_lm import CharTokenizer\n",
    "from data.dataset import TypoDataset\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from data.dataset import TypoOnlineDataset\n",
    "from fastDamerauLevenshtein import damerauLevenshtein\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import pipeline, AutoModelForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert/ncbi_bert_base/pytorch_model.bin were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SpellCorrectionModel(config_file=\"/bert_config.json\")\n",
    "typo_tokenizer = CharTokenizer()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "writer = SummaryWriter(log_dir='logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file data/mimic_synthetic/val.tsv... 10000 rows\n",
      "Parsing rows (10 processes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1098.78it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TypoOnlineDataset(\"data/mimic3/split\", \"data/lexicon/lexicon_en.json\", model.tokenizer, typo_tokenizer,2)\n",
    "dataset_val = TypoDataset(os.path.join(\"data/mimic_synthetic\", 'val.tsv'), model.tokenizer, typo_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss function with misspelling penalty\n",
    "'''\n",
    "def loss_function(probabilities, correct_label, predicted_spellings, correct_spellings):\n",
    "    loss = torch.nn.functional.cross_entropy(probabilities.view(-1, model.tokenizer.vocab_size), correct_label.view(-1))\n",
    "    distance = damerauLevenshtein(' '.join(predicted_spellings), ' '.join(correct_spellings))\n",
    "    total_loss = loss + 0.5 * distance\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                num_workers=0,\n",
    "                                                collate_fn=dataset_train.get_collate_fn())\n",
    "\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val,\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=False,\n",
    "                                                     drop_last=True,\n",
    "                                                     num_workers=0,\n",
    "                                                     collate_fn=dataset_val.get_collate_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "\n",
    "num_epochs = 5\n",
    "warmup_proportion = 0.1\n",
    "num_training_steps = 10000\n",
    "\n",
    "accelerator = Accelerator()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = num_training_steps * num_epochs\n",
    "warmup_steps = int(warmup_proportion * total_steps)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, dataloader_train, dataloader_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "output_dir = \"bluebert-finetuned-mimic-v1\"\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=total_steps\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f241fa01b9c451cb6baa77f938831fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 0.0490\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a424cb2f7c84633aef2900195301cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train loss: 0.0525\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9e3c4c6b0c46cd844e63e7ca88cbdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train loss: 0.0468\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed7b4a4346a41b3b00536567ce97e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train loss: 0.0464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d71cf991fc2476484dd253b01ffcae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, train loss: 0.0349\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "train_iter = iter(dataloader_train)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(range(1000))\n",
    "    global_step = 0\n",
    "\n",
    "    while global_step < 1000:\n",
    "        # Training\n",
    "        batch = next(train_iter)\n",
    "        input_ids = batch[\"context_tokens\"]\n",
    "        attention_mask = batch['context_attention_mask']\n",
    "        #print(attention_mask.dtype)\n",
    "        misspelling = batch['typo']\n",
    "        correct_spelling = batch['correct']\n",
    "        \n",
    "        outputs, prediction = model.forward(input_ids, attention_mask, misspelling, correct_spelling)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = model.loss\n",
    "        train_loss += loss.item()\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(BATCH_SIZE)\n",
    "        # Save and store\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.bert.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        unwrapped_model.config.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        unwrapped_model.tokenizer.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            model.bert.save_pretrained(output_dir)\n",
    "            model.config.save_pretrained(output_dir)\n",
    "            model.tokenizer.save_pretrained(output_dir)\n",
    "            lr_scheduler.step()\n",
    "        global_step += BATCH_SIZE\n",
    "    train_loss /= (1000/BATCH_SIZE)\n",
    "    writer.add_scalar('Loss', train_loss, global_step=epoch)\n",
    "    print(f\"Epoch {epoch+1}, train loss: {train_loss:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient mentioned she took tylenol for the.\n",
      "patient mentioned she took tylenol for the?\n",
      "patient mentioned she took tylenol for the pain\n",
      "patient mentioned she took tylenol for the headache\n",
      "patient mentioned she took tylenol for the fever\n",
      "patient mentioned she took tylenol for the rash\n",
      "patient mentioned she took tylenol for the discomfort\n",
      "patient mentioned she took tylenol for the ha\n",
      "patient mentioned she took tylenol for the cough\n",
      "patient mentioned she took tylenol for the same\n",
      "patient mentioned she took tylenol for the past\n",
      "patient mentioned she took tylenol for the day\n",
      "patient mentioned she took tylenol for the night\n",
      "patient mentioned she took tylenol for the swelling\n",
      "patient mentioned she took tylenol for the nausea\n",
      "patient mentioned she took tylenol for the family\n",
      "patient mentioned she took tylenol for the flu\n",
      "patient mentioned she took tylenol for the last\n",
      "patient mentioned she took tylenol for the morning\n",
      "patient mentioned she took tylenol for the am\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForMaskedLM\n",
    "\n",
    "model_trained = AutoModelForMaskedLM.from_pretrained(\"./bluebert-finetuned-mimic\")\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=output_dir, top_k= 20\n",
    ")\n",
    "\n",
    "preds = mask_filler(\"Patient mentioned she took tylenol for the [MASK]\")\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\"{pred['sequence']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'apple'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = input()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10  (repro-dlh)",
   "language": "python",
   "name": "repro-dlh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
