{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: fairseq in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: transformers in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (4.28.1)\n",
      "Requirement already satisfied: dill in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (0.3.6)\n",
      "Requirement already satisfied: fastDamerauLevenshtein in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (1.0.7)\n",
      "Requirement already satisfied: tensorboardX in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (2.6)\n",
      "Requirement already satisfied: accelerate in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (0.18.0)\n",
      "Collecting textdistance\n",
      "  Downloading textdistance-4.5.0-py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: filelock in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: cffi in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from fairseq) (1.15.1)\n",
      "Requirement already satisfied: cython in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from fairseq) (0.29.34)\n",
      "Requirement already satisfied: hydra-core<1.1,>=1.0.7 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from fairseq) (1.0.7)\n",
      "Requirement already satisfied: omegaconf<2.1 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from fairseq) (2.0.6)\n",
      "Requirement already satisfied: regex in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from fairseq) (2023.5.5)\n",
      "Requirement already satisfied: sacrebleu>=1.4.12 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from fairseq) (2.3.1)\n",
      "Requirement already satisfied: tqdm in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from fairseq) (4.65.0)\n",
      "Requirement already satisfied: bitarray in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from fairseq) (2.7.3)\n",
      "Requirement already satisfied: torchaudio>=0.8.0 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from fairseq) (2.0.1)\n",
      "Requirement already satisfied: numpy in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from fairseq) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: protobuf<4,>=3.8.0 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from tensorboardX) (3.20.3)\n",
      "Requirement already satisfied: psutil in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: fsspec in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from hydra-core<1.1,>=1.0.7->fairseq) (4.8)\n",
      "Requirement already satisfied: portalocker in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (2.7.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
      "Requirement already satisfied: colorama in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (0.4.6)\n",
      "Requirement already satisfied: lxml in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from sacrebleu>=1.4.12->fairseq) (4.9.2)\n",
      "Requirement already satisfied: pycparser in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from cffi->fairseq) (2.21)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/tamuneke/miniconda3/envs/repro-dlh/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: textdistance\n",
      "Successfully installed textdistance-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch fairseq transformers dill fastDamerauLevenshtein tensorboardX accelerate textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.dataset import TypoDataset, TypoOnlineDataset\n",
    "from model.char_lm import CharacterLanguageModel, CharTokenizer, CrossEntropyLoss, Trie\n",
    "from model.parallel import DataParallelModel, DataParallelCriterion\n",
    "from utils.checkpoint_manager import CheckPointManager\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "from fastDamerauLevenshtein import damerauLevenshtein\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from model.word_lm import SpellCorrectionModel\n",
    "from model.char_lm import CharTokenizer\n",
    "from data.dataset import TypoDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert/ncbi_bert_base/pytorch_model.bin were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SpellCorrectionModel(config_file=\"/bert_config.json\")\n",
    "typo_tokenizer = CharTokenizer()\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "writer = SummaryWriter(log_dir='logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read file data/mimic_synthetic/val.tsv... 10000 rows\n",
      "Parsing rows (10 processes)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:09<00:00, 1101.42it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = TypoOnlineDataset(\"data/mimic3/split\", \"data/lexicon/lexicon_en.json\", model.tokenizer, typo_tokenizer,2)\n",
    "dataset_val = TypoDataset(os.path.join(\"data/mimic_synthetic\", 'val.tsv'), model.tokenizer, typo_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss function with misspelling penalty\n",
    "'''\n",
    "def loss_function(probabilities, correct_label, predicted_spellings, correct_spellings):\n",
    "    # Calculate cross-entropy loss\n",
    "    loss = torch.nn.functional.cross_entropy(probabilities.view(-1, model.tokenizer.vocab_size), correct_label.view(-1))\n",
    "    #Compute fastDamerauLevenshtein distance between predicted and correct spellings\n",
    "    distance = damerauLevenshtein(' '.join(predicted_spellings), ' '.join(correct_spellings))\n",
    "    #print(distance)\n",
    "    total_loss = loss + 0.5 * distance\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                num_workers=0,\n",
    "                                                collate_fn=dataset_train.get_collate_fn())\n",
    "\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val,\n",
    "                                                     batch_size=BATCH_SIZE,\n",
    "                                                     shuffle=False,\n",
    "                                                     drop_last=True,\n",
    "                                                     num_workers=0,\n",
    "                                                     collate_fn=dataset_val.get_collate_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from accelerate import Accelerator\n",
    "\n",
    "num_epochs = 5\n",
    "warmup_proportion = 0.1\n",
    "num_training_steps = 10000\n",
    "\n",
    "accelerator = Accelerator()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = num_training_steps * num_epochs\n",
    "warmup_steps = int(warmup_proportion * total_steps)\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, dataloader_train, dataloader_val\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "output_dir = \"bluebert-finetuned-mimic-v1\"\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps, \n",
    "    num_training_steps=total_steps\n",
    "   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdf3e32e9bd4107886fb873c0073767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 0.0357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4673e44486b54a9499207fe61721c2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train loss: 0.0373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0e527685a84e5cbe77329ae45c213a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train loss: 0.0350\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44cdbabb95e481aa041e4d9cf3a9536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train loss: 0.0376\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d4511672ed4c479f15a20299fda7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, train loss: 0.0350\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "model.train()\n",
    "criteria = CrossEntropyLoss().to(device)\n",
    "\n",
    "train_iter = iter(dataloader_train)\n",
    "MODE = \"Train\"\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    progress_bar = tqdm(range(1000))\n",
    "    global_step = 0\n",
    "\n",
    "    while global_step < 1000:\n",
    "        # Training\n",
    "        if MODE == \"Train\":\n",
    "            batch = next(train_iter)\n",
    "            #print(batch)\n",
    "            input_ids = batch[\"context_tokens\"]\n",
    "            attention_mask = batch['context_attention_mask']\n",
    "            #print(attention_mask.dtype)\n",
    "            misspelling = batch['typo']\n",
    "            correct_spelling = batch['correct']\n",
    "            \n",
    "            outputs, prediction = model.forward(input_ids, attention_mask, misspelling, correct_spelling)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = model.loss\n",
    "            train_loss += loss.item()\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(BATCH_SIZE)\n",
    "            # if global_step % 2 == 0:\n",
    "            #     print(f' Training: Epoch {global_step/BATCH_SIZE}, Batch {global_step}, Loss {loss.item()} Did Reduce: {prev > loss.item()}')\n",
    "            #     prev = loss.item()\n",
    "                \n",
    "        # Save and store\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.bert.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        unwrapped_model.config.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "        unwrapped_model.tokenizer.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            model.bert.save_pretrained(output_dir)\n",
    "            model.config.save_pretrained(output_dir)\n",
    "            model.tokenizer.save_pretrained(output_dir)\n",
    "            lr_scheduler.step()\n",
    "        global_step += BATCH_SIZE\n",
    "    train_loss /= (1000/BATCH_SIZE)\n",
    "    writer.add_scalar('Loss', train_loss, global_step=epoch)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, train loss: {train_loss:.4f}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bluebert-finetuned-mimic-v1'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpellCorrectionModel(\n",
       "  (bert): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (dropout): Dropout(p=0.1, inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"bluebert-finetuned-mimic\"\n",
    "model = SpellCorrectionModel(NCBI_BERT = output_dir, config_file= \"/config.json\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='mps:0')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids = torch.tensor(model.tokenizer.convert_tokens_to_ids(correct_spelling))\n",
    "label[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 130]), torch.Size([100]), torch.Size([100, 128]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape, target_ids.shape, input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f66b33b8de4c2f93122f7b51eb0c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "SpellCorrectionModel.forward() missing 1 required positional argument: 'correct_spelling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m correct_spelling \u001b[39m=\u001b[39m batch_val[\u001b[39m'\u001b[39m\u001b[39mcorrect\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 23\u001b[0m     outputs,  prediction, label \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(input_ids, attention_mask, misspelling)\n\u001b[1;32m     24\u001b[0m global_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(correct_spelling)\n\u001b[1;32m     25\u001b[0m current_correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: SpellCorrectionModel.forward() missing 1 required positional argument: 'correct_spelling'"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "val_iter = iter(dataloader_val)\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "model.eval()\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "global_total = 0\n",
    "global_correct = 0\n",
    "while global_step < num_training_steps:\n",
    "    # Evaluation\n",
    "    try:\n",
    "        batch_val = next(val_iter)\n",
    "    except StopIteration:\n",
    "        val_iter = iter(dataloader_val)\n",
    "        batch_val = next(val_iter)\n",
    "    input_ids = batch_val[\"context_tokens\"]\n",
    "    attention_mask = batch_val['context_attention_mask']\n",
    "    misspelling = batch_val['typo']\n",
    "    correct_spelling = batch_val['correct']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs,  prediction, label = model.forward(input_ids, attention_mask, misspelling)\n",
    "    global_total += len(correct_spelling)\n",
    "    current_correct = 0\n",
    "    for index in range(len(correct_spelling)):\n",
    "        if correct_spelling[index] == prediction[index]:\n",
    "            global_correct +=1\n",
    "        \n",
    "    loss = loss_function(outputs, label, misspelling, correct_spelling).to(device)\n",
    "    loss = loss.requires_grad_(True)\n",
    "    accelerator.backward(loss)\n",
    "\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update(1)\n",
    "    if global_step %50 == 0:\n",
    "        print(f'Total/Correct = {global_total} / {global_correct}')\n",
    "    global_step+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> aorta : mildly dilated aortic root. focal calcifications in aortic root.\n",
      ">>> aorta : mildly dilated aortic arch. focal calcifications in aortic root.\n",
      ">>> aorta : mildly dilated aortic diameter. focal calcifications in aortic root.\n",
      ">>> aorta : mildly dilated aorticrta. focal calcifications in aortic root.\n",
      ">>> aorta : mildly dilated aortic ascending. focal calcifications in aortic root.\n",
      ">>> aorta : mildly dilated aortic cavity. focal calcifications in aortic root.\n",
      ">>> aorta : mildly dilated aortic ao. focal calcifications in aortic root.\n",
      ">>> aorta : mildly dilated aortic roots. focal calcifications in aortic root.\n",
      ">>> aorta : mildly dilated aortic artery. focal calcifications in aortic root.\n",
      ">>> aorta : mildly dilated aortic valve. focal calcifications in aortic root.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model_trained = AutoModelForMaskedLM.from_pretrained(\"./bluebert-finetuned-mimic\")\n",
    "\n",
    "mask_filler = pipeline(\n",
    "    \"fill-mask\", model=output_dir, top_k= 10\n",
    ")\n",
    "\n",
    "preds = mask_filler(\"AORTA: Mildly dilated aortic [MASK]. Focal calcifications in aortic root.\")\n",
    "\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2180.1428571428573"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30522/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 563,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "damerauLevenshtein(\"she\", \"moom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iter = iter(train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='/Users/tamuneke/UIUC/DLH/cim-misspelling-dlh-repro/bert/ncbi_bert_base/', vocab_size=30522, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10  (repro-dlh)",
   "language": "python",
   "name": "repro-dlh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
